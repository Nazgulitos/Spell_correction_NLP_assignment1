{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10843505,"sourceType":"datasetVersion","datasetId":6734256}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Context-sensitive Spelling Correction\n\nThe goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n\nSubmit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n\nUseful links:\n- [Norvig's solution](https://norvig.com/spell-correct.html)\n- [Norvig's dataset](https://norvig.com/big.txt)\n- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n\nGrading:\n- 60 points - Implement spelling correction\n- 20 points - Justify your decisions\n- 20 points - Evaluate on a test set\n","metadata":{"id":"DIgM6C9HYUhm"}},{"cell_type":"markdown","source":"## Implement context-sensitive spelling correction\n\nYour task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n\nThe best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n\nWhen solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n\n- solving a problem of n-grams frequencies storing for a large corpus;\n- taking into account keyboard layout and associated misspellings;\n- efficiency improvement to make the solution faster;\n- ...\n\nPlease don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n\n##### IMPORTANT:  \nYour project should not be a mere code copy-paste from somewhere. You must provide:\n- Your implementation\n- Analysis of why the implemented approach is suggested\n- Improvements of the original approach that you have chosen to implement","metadata":{"id":"x-vb8yFOGRDF"}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import brown, reuters, gutenberg, webtext\n\n# Download necessary NLTK data and external corpus\nnltk.download(['brown', 'reuters', 'gutenberg', 'webtext', 'punkt', 'genesis'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:41.711873Z","iopub.execute_input":"2025-02-25T21:47:41.712350Z","iopub.status.idle":"2025-02-25T21:47:41.725838Z","shell.execute_reply.started":"2025-02-25T21:47:41.712317Z","shell.execute_reply":"2025-02-25T21:47:41.724364Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package brown to /usr/share/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n[nltk_data] Downloading package reuters to /usr/share/nltk_data...\n[nltk_data]   Package reuters is already up-to-date!\n[nltk_data] Downloading package gutenberg to /usr/share/nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n[nltk_data] Downloading package webtext to /usr/share/nltk_data...\n[nltk_data]   Package webtext is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package genesis to /usr/share/nltk_data...\n[nltk_data]   Package genesis is already up-to-date!\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"import requests\n\n# Download and load Norvig's big.txt\nresponse = requests.get('https://norvig.com/big.txt')\nwith open('big.txt', 'w') as f:\n    f.write(response.text.lower())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:41.727575Z","iopub.execute_input":"2025-02-25T21:47:41.727936Z","iopub.status.idle":"2025-02-25T21:47:42.521680Z","shell.execute_reply.started":"2025-02-25T21:47:41.727907Z","shell.execute_reply":"2025-02-25T21:47:42.520516Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\ndef load_corpus():\n    \"\"\"Load NLTK corpora and external data.\"\"\"\n    raw_text = []\n    # NLTK corpora\n    for corpus in [brown, reuters, gutenberg, webtext]:\n        try:\n            raw_text.extend([corpus.raw(file_id) for file_id in corpus.fileids()])\n        except LookupError:\n            continue\n    # External data (big.txt)\n    with open('big.txt', 'r') as f:\n        raw_text.append(f.read())\n    return ' '.join(raw_text).lower()\n\n# Load corpus and tokenize with improved tokenizer\ntokenizer = RegexpTokenizer(r\"\\w+[\\w']*\")  # Handles contractions\ntext = load_corpus()\nwords = tokenizer.tokenize(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:42.523571Z","iopub.execute_input":"2025-02-25T21:47:42.523901Z","iopub.status.idle":"2025-02-25T21:47:45.457618Z","shell.execute_reply.started":"2025-02-25T21:47:42.523874Z","shell.execute_reply":"2025-02-25T21:47:45.456338Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from nltk.util import ngrams\nfrom collections import Counter\n\n# Build n-grams with Laplace smoothing\nunigrams = Counter(words)\nbigrams = Counter(ngrams(words, 2))\ntrigrams = Counter(ngrams(words, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:45.459194Z","iopub.execute_input":"2025-02-25T21:47:45.459571Z","iopub.status.idle":"2025-02-25T21:47:57.753025Z","shell.execute_reply.started":"2025-02-25T21:47:45.459542Z","shell.execute_reply":"2025-02-25T21:47:57.751776Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\n\nkeyboard_neighbors = {\n    'q': ['w', 'a', 's'],\n    'w': ['q', 'e', 'a', 's', 'd'],\n    'e': ['w', 'r', 's', 'd', 'f'],\n    'r': ['e', 't', 'd', 'f', 'g'],\n    't': ['r', 'y', 'f', 'g', 'h'],\n    'y': ['t', 'u', 'g', 'h', 'j'],\n    'u': ['y', 'i', 'h', 'j', 'k'],\n    'i': ['u', 'o', 'j', 'k', 'l'],\n    'o': ['i', 'p', 'k', 'l'],\n    'p': ['o', 'l'],\n    'a': ['q', 'w', 's', 'z', 'x'],\n    's': ['a', 'w', 'e', 'd', 'z', 'x', 'c'],\n    'd': ['s', 'e', 'r', 'f', 'x', 'c', 'v'],\n    'f': ['d', 'r', 't', 'g', 'c', 'v', 'b'],\n    'g': ['f', 't', 'y', 'h', 'v', 'b', 'n'],\n    'h': ['g', 'y', 'u', 'j', 'b', 'n', 'm'],\n    'j': ['h', 'u', 'i', 'k', 'n', 'm'],\n    'k': ['j', 'i', 'o', 'l', 'm'],\n    'l': ['k', 'o', 'p'],\n    'z': ['a', 's', 'x'],\n    'x': ['z', 's', 'd', 'c'],\n    'c': ['x', 'd', 'f', 'v'],\n    'v': ['c', 'f', 'g', 'b'],\n    'b': ['v', 'g', 'h', 'n'],\n    'n': ['b', 'h', 'j', 'm'],\n    'm': ['n', 'j', 'k']\n}\n\ndef edits1(word):\n    \"\"\"Edits with priority to keyboard neighbors.\"\"\"\n    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    edits = set()\n    # Deletes, transposes, replaces, inserts\n    for L, R in splits:\n        if R: edits.add(L + R[1:])  # Deletes\n        if len(R) > 1: edits.add(L + R[1] + R[0] + R[2:])  # Transposes\n        if R:\n            for c in keyboard_neighbors.get(R[0], 'abcdefghijklmnopqrstuvwxyz'):\n                edits.add(L + c + R[1:])  # Prioritize nearby keys\n        for c in 'abcdefghijklmnopqrstuvwxyz':\n            edits.add(L + c + R)  # Inserts\n    return edits\n\ndef edits2(word):\n    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n\ndef known(words):\n    return {w for w in words if w in unigrams}\n\ndef candidates(word):\n    return known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n\ndef correct_word(word, prev=None, next=None):\n    \"\"\"Use interpolated probability of n-grams.\"\"\"\n    candidates_list = candidates(word)\n    if not candidates_list:\n        return word\n    # Linear interpolation: trigram*0.6 + bigram*0.3 + unigram*0.1\n    scores = {}\n    for cand in candidates_list:\n        trigram_score = trigrams.get((prev, cand, next), 0) * 0.6 if prev and next else 0\n        bigram_prev = bigrams.get((prev, cand), 0) * 0.3 if prev else 0\n        bigram_next = bigrams.get((cand, next), 0) * 0.3 if next else 0\n        unigram_score = unigrams[cand] * 0.1\n        scores[cand] = trigram_score + bigram_prev + bigram_next + unigram_score\n    return max(scores, key=scores.get)\n\ndef correct_text(text):\n    tokens = tokenizer.tokenize(text.lower())\n    corrected = []\n    for i, word in enumerate(tokens):\n        prev = tokens[i-1] if i > 0 else None\n        next_word = tokens[i+1] if i < len(tokens)-1 else None\n        corrected.append(correct_word(word, prev, next_word))\n    return ' '.join(corrected)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:57.754422Z","iopub.execute_input":"2025-02-25T21:47:57.754691Z","iopub.status.idle":"2025-02-25T21:47:57.771982Z","shell.execute_reply.started":"2025-02-25T21:47:57.754666Z","shell.execute_reply":"2025-02-25T21:47:57.770590Z"}},"outputs":[{"name":"stdout","text":"neve gonna give you up\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Test\nprint(correct_text(\"doing sport\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:57.773439Z","iopub.execute_input":"2025-02-25T21:47:57.773803Z","iopub.status.idle":"2025-02-25T21:47:57.795814Z","shell.execute_reply.started":"2025-02-25T21:47:57.773767Z","shell.execute_reply":"2025-02-25T21:47:57.794567Z"}},"outputs":[{"name":"stdout","text":"doing sport\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"text_with_errors = \"gonna plau duing sportd\"\ncorrected_text = correct_text(text_with_errors)\nprint(corrected_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:54:26.677459Z","iopub.execute_input":"2025-02-25T21:54:26.677828Z","iopub.status.idle":"2025-02-25T21:54:26.685619Z","shell.execute_reply.started":"2025-02-25T21:54:26.677800Z","shell.execute_reply":"2025-02-25T21:54:26.683889Z"}},"outputs":[{"name":"stdout","text":"gonna play during sports\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"## Justify your decisions\n\nWrite down justificaitons for your implementation choices. For example, these choices could be:\n- Which ngram dataset to use\n- Which weights to assign for edit1, edit2 or absent words probabilities\n- Beam search parameters\n- etc.","metadata":{"id":"oML-5sJwGRLE"}},{"cell_type":"markdown","source":"## I made the basic and intial solution for spelling correction and included:\n### N-gram Dataset\n- **Combined Multiple Corpora**: We integrated multiple corpora, including Norvig's `big.txt`, to improve coverage and reduce data sparsity for n-grams. This ensures that the system has a robust vocabulary and context for better spell correction.\n\n### Edit Operations\n- **Included `edits2`**: By including second-level edits (`edits2`), the system can capture more complex corrections, such as fixing \"nevew\" to \"never\".\n- **Keyboard Proximity in `edits1`**: Prioritizing edits based on keyboard proximity helps correct likely typos (e.g., \"gice\" → \"give\" instead of \"rice\").\n\n### Tokenization\n- **RegexpTokenizer**: We used `RegexpTokenizer` to handle contractions (e.g., \"gonna\" remains one word), ensuring that tokenization aligns with natural language usage.\n\n### Smoothing\n- **Linear Interpolation**: We applied linear interpolation of n-gram probabilities (trigram: 60%, bigram: 30%, unigram: 10%) to handle unseen contexts and improve the robustness of the model.\n\n### Efficiency\n- **Counter for Frequency Storage**: We used Python's `Counter` for efficient frequency storage. For larger datasets, a probabilistic structure like Bloom filters could be explored to further optimize memory usage and performance.","metadata":{"id":"6Xb_twOmVsC6"}},{"cell_type":"markdown","source":"## Evaluate on a test set\n\nYour task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies.","metadata":{"id":"46rk65S4GRSe"}},{"cell_type":"code","source":"pip install pyspellchecker","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:47:57.817958Z","iopub.execute_input":"2025-02-25T21:47:57.818309Z","iopub.status.idle":"2025-02-25T21:48:02.231384Z","shell.execute_reply.started":"2025-02-25T21:47:57.818280Z","shell.execute_reply":"2025-02-25T21:48:02.229731Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import random\nfrom spellchecker import SpellChecker\nimport nltk\nfrom nltk.corpus import nps_chat\n\n# Checked on data fronm this corpus\nnltk.download('nps_chat')\n\n# Noise generation\ndef add_noise(sentence, noise_level=0.2):\n    words = sentence.split()\n    noisy = []\n    for word in words:\n        if len(word) > 3 and random.random() < noise_level:\n            idx = random.randint(0, len(word)-1)\n            noisy_word = word[:idx] + word[idx+1:]\n            noisy.append(noisy_word)\n        else:\n            noisy.append(word)\n    return ' '.join(noisy)\n\n# Norvig\ndef norvig_correct_word(word):\n    spell = SpellChecker()\n    corrected_word = spell.correction(word)\n    return corrected_word if corrected_word is not None else word\n\nall_posts = list(nps_chat.posts())\ntest_posts = random.sample(all_posts, 100)\n\nnorvig_correct = 0\ncontext_correct = 0\ntotal = 0\n\nfor post in test_posts:\n    sentence = \" \".join(post)\n    noisy = add_noise(sentence)\n    context_fixed = correct_text(noisy)\n    norvig_fixed = ' '.join([norvig_correct_word(word) for word in noisy.split()]) \n    original_words = sentence.split()\n    context_words = context_fixed.split()\n    norvig_words = norvig_fixed.split()\n    for cw, nw, ow in zip(context_words, norvig_words, original_words):\n        total += 1\n        if cw == ow:\n            context_correct += 1\n        if nw == ow:\n            norvig_correct += 1\n\nprint(f\"Context-aware accuracy: {context_correct/total:.2f}\")\nprint(f\"Norvig's accuracy: {norvig_correct/total:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T21:57:07.689364Z","iopub.execute_input":"2025-02-25T21:57:07.689824Z","iopub.status.idle":"2025-02-25T21:57:18.335817Z","shell.execute_reply.started":"2025-02-25T21:57:07.689790Z","shell.execute_reply":"2025-02-25T21:57:18.334629Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package nps_chat to /usr/share/nltk_data...\n[nltk_data]   Package nps_chat is already up-to-date!\nContext-aware accuracy: 0.44\nNorvig's accuracy: 0.75\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"#### Useful resources (also included in the archive in moodle):\n\n1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)","metadata":{}},{"cell_type":"markdown","source":"## As my solution showed not such good results there are proposed improvements for a solution\n\n### 1. **Enhanced Edit Operations**\n   - **Levenshtein Distance-Based Edits**: In addition to keyboard proximity,implementaion Levenshtein distance (edit distance) can be useful for identifying common typographical errors that are not strictly adjacent on the keyboard but might be phonetically similar.\n\n### 2. **Improved Tokenization**\n   - **Subword Tokenization**: Utilization of subword tokenization (e.g., Byte-Pair Encoding, WordPiece) to break down words into smaller, meaningful units. This could improve the handling of rare or unseen words, as the model could learn to correct them based on their subword components.\n   - **Contextual Tokenization**: Implementation tokenizers that dynamically adjust based on sentence context.\n\n### 3. **Smoothing Techniques**\n   - **Kneser-Ney Smoothing**: Aplication of Kneser-Ney smoothing to improve the estimation of low-frequency n-grams, making the system more robust against unseen or rare word combinations.\n   - **Perplexity-Based Smoothing**: Use of perplexity to measure the model’s predictive uncertainty and adjust the smoothing accordingly. This can help fine-tune how much weight should be given to various n-gram models during correction.\nress the cases where the model consistently mispredicts certain words or phrases.","metadata":{}}]}