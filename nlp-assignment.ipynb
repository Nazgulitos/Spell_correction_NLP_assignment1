{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10843505,"sourceType":"datasetVersion","datasetId":6734256}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 1: Context-sensitive Spelling Correction\n### Author: Nazgul Salikhova\n### Group: B22-AAI-02\n### Email: n.salikhova@innopolis.university","metadata":{}},{"cell_type":"markdown","source":"# Task: Context-sensitive Spelling Correction\n\nThe goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n\nSubmit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n\nUseful links:\n- [Norvig's solution](https://norvig.com/spell-correct.html)\n- [Norvig's dataset](https://norvig.com/big.txt)\n- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n\nGrading:\n- 60 points - Implement spelling correction\n- 20 points - Justify your decisions\n- 20 points - Evaluate on a test set\n","metadata":{"id":"DIgM6C9HYUhm"}},{"cell_type":"markdown","source":"## Implement context-sensitive spelling correction\n\nYour task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n\nThe best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n\nWhen solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n\n- solving a problem of n-grams frequencies storing for a large corpus;\n- taking into account keyboard layout and associated misspellings;\n- efficiency improvement to make the solution faster;\n- ...\n\nPlease don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n\n##### IMPORTANT:  \nYour project should not be a mere code copy-paste from somewhere. You must provide:\n- Your implementation\n- Analysis of why the implemented approach is suggested\n- Improvements of the original approach that you have chosen to implement","metadata":{"id":"x-vb8yFOGRDF"}},{"cell_type":"markdown","source":"# Solution for a custom Context-sensitive Spell Corrector","metadata":{}},{"cell_type":"markdown","source":"## 1. Download and Processing Corpus data\nThe bigrams.txt and fivegrams.txt files contain n-gram frequency counts, which are used to analyze word co-occurrences in a corpus. These files are processed to extract bigram (2-word) and fivegram (5-word) frequency distributions. The extracted frequencies are then moved to the GPU to speed up computations in downstream tasks.\n\nThe were problems with encoding, so it was decided to use chardet to automatically detect the correct file encoding before reading the data. This ensures compatibility and prevents errors when loading n-gram frequency counts from different text sources.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport re\nimport torch\nimport chardet\n\ndef load_ngram_data(file_path, n):\n    \"\"\"Loads n-gram frequency data from a text file.\"\"\"\n    ngram_counts = defaultdict(int)\n    \n    # Detect encoding\n    with open(file_path, 'rb') as file:\n        raw_data = file.read()\n        result = chardet.detect(raw_data)\n        encoding = result['encoding']\n    \n    # Load data with detected encoding\n    with open(file_path, 'r', encoding=encoding) as file:\n        for line in file:\n            parts = line.strip().split()\n            if len(parts) < n + 1:\n                continue\n            ngram = tuple(parts[1:])  # The words forming the n-gram\n            count = int(parts[0])  # Frequency of n-gram occurrence \n            ngram_counts[ngram] = count\n    return ngram_counts\n\n# Load bigram and fivegram data\nbigrams = load_ngram_data(\"/kaggle/input/given-data/useful data/bigrams (2).txt\", 2)\nfivegrams = load_ngram_data(\"/kaggle/input/given-data/useful data/fivegrams (2).txt\", 5)\n\n# # Move bigram/fivegram frequencies to GPU for faster access\n# bigram_values = torch.tensor(list(bigrams.values()), dtype=torch.float32, device=\"cuda\")\n# fivegram_values = torch.tensor(list(fivegrams.values()), dtype=torch.float32, device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:37:47.840560Z","iopub.execute_input":"2025-03-04T20:37:47.840945Z","iopub.status.idle":"2025-03-04T20:39:17.858962Z","shell.execute_reply.started":"2025-03-04T20:37:47.840911Z","shell.execute_reply":"2025-03-04T20:39:17.857962Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# %pip install wikipedia-api nltk -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T23:27:41.933258Z","iopub.execute_input":"2025-03-03T23:27:41.933538Z","iopub.status.idle":"2025-03-03T23:27:47.201508Z","shell.execute_reply.started":"2025-03-03T23:27:41.933518Z","shell.execute_reply":"2025-03-03T23:27:47.200593Z"}},"outputs":[{"name":"stdout","text":"Collecting wikipedia-api\n  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2025.1.31)\nBuilding wheels for collected packages: wikipedia-api\n  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=d6387a8b27d5442486d0c54862f5b99aa6da5cd9d5bdfe9fbdc66f6b4bc96ce4\n  Stored in directory: /root/.cache/pip/wheels/1d/f8/07/0508c38722dcd82ee355e9d85e33c9e9471d4bec0f8ae72de0\nSuccessfully built wikipedia-api\nInstalling collected packages: wikipedia-api\nSuccessfully installed wikipedia-api-0.8.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"# %pip install ngram -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:38:13.238091Z","iopub.execute_input":"2025-03-04T13:38:13.238376Z","iopub.status.idle":"2025-03-04T13:38:17.542477Z","shell.execute_reply.started":"2025-03-04T13:38:13.238341Z","shell.execute_reply":"2025-03-04T13:38:17.541578Z"}},"outputs":[{"name":"stdout","text":"Collecting ngram\n  Downloading ngram-4.0.3-py3-none-any.whl.metadata (3.5 kB)\nDownloading ngram-4.0.3-py3-none-any.whl (24 kB)\nInstalling collected packages: ngram\nSuccessfully installed ngram-4.0.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# from collections import defaultdict\n# import nltk\n# nltk.download('words')\n# from nltk.corpus import words\n\n# # Initialize dictionary with English words\n# dictionary = set(words.words())  # Contains a large list of valid English words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T13:38:17.543509Z","iopub.execute_input":"2025-03-04T13:38:17.543858Z","iopub.status.idle":"2025-03-04T13:38:19.158686Z","shell.execute_reply.started":"2025-03-04T13:38:17.543824Z","shell.execute_reply":"2025-03-04T13:38:19.157904Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Enhance Dictionary with NLTK Words and Compute Word Frequencies  \n\n- **NLTK Words Integration**:  \n  - The NLTK `words` corpus is downloaded and added to the existing dictionary.  \n  - The vocabulary includes commonly used English words, improving word recognition and correction accuracy.  \n\n- **Word Frequency Calculation**:  \n  - The frequency of each word is computed from both **bigrams** and **fivegrams**.  \n  - Each word’s occurrence count is aggregated based on how often it appears in the n-grams.  \n  - This frequency data helps prioritize more common words during spell-checking or text analysis.  ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import words\nnltk.download('words')\n\n# Add NLTK words to your dictionary\nenglish_vocab = set(words.words())\ndictionary = dictionary = set(words.words())\n\n# Load word frequencies\nword_frequencies = defaultdict(int)\nfor bigram, count in bigrams.items():\n    for word in bigram:\n        word_frequencies[word] += count\n\nfor fivegram, count in fivegrams.items():\n    for word in fivegram:\n        word_frequencies[word] += count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:43:05.841538Z","iopub.execute_input":"2025-03-04T20:43:05.841941Z","iopub.status.idle":"2025-03-04T20:43:08.015833Z","shell.execute_reply.started":"2025-03-04T20:43:05.841912Z","shell.execute_reply":"2025-03-04T20:43:08.014799Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package words to /usr/share/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Compute Levenshtein Distance between words in Corpus\n\n- **keyboard_neighbors**: This dictionary defines a mapping of each letter to its neighboring keys on a QWERTY keyboard. It helps account for common typing errors due to adjacent key presses.\n- **Function:** `edit_distance(word1, word2))` This function calculates the Levenshtein distance between two words, with special handling for keyboard typos and missing letters.  \n- A standard Levenshtein distance approach is used, but with modifications:  \n  - **Substitutions** have a lower cost (0.5) if the characters involved are keyboard neighbors.  \n  - **Insertions** also have a lower cost (0.5) if they result in a valid word.  \n  - **Deletions** follow the standard cost of 1.  \n\nThis method is useful for spell-checking because it allows for more forgiving error correction by considering common typing mistakes.","metadata":{}},{"cell_type":"code","source":"# QWERTY Keyboard neighbor mappings\nkeyboard_neighbors = {\n    'q': ['w', 'a', 's'],\n    'w': ['q', 'e', 'a', 's', 'd'],\n    'e': ['w', 'r', 's', 'd', 'f'],\n    'r': ['e', 't', 'd', 'f', 'g'],\n    't': ['r', 'y', 'f', 'g', 'h'],\n    'y': ['t', 'u', 'g', 'h', 'j'],\n    'u': ['y', 'i', 'h', 'j', 'k'],\n    'i': ['u', 'o', 'j', 'k', 'l'],\n    'o': ['i', 'p', 'k', 'l'],\n    'p': ['o', 'l'],\n    'a': ['q', 'w', 's', 'z', 'x'],\n    's': ['a', 'w', 'e', 'd', 'z', 'x', 'c'],\n    'd': ['s', 'e', 'r', 'f', 'x', 'c', 'v'],\n    'f': ['d', 'r', 't', 'g', 'c', 'v', 'b'],\n    'g': ['f', 't', 'y', 'h', 'v', 'b', 'n'],\n    'h': ['g', 'y', 'u', 'j', 'b', 'n', 'm'],\n    'j': ['h', 'u', 'i', 'k', 'n', 'm'],\n    'k': ['j', 'i', 'o', 'l', 'm', 'y'],\n    'l': ['k', 'o', 'p'],\n    'z': ['a', 's', 'x'],\n    'x': ['z', 's', 'd', 'c'],\n    'c': ['x', 'd', 'f', 'v'],\n    'v': ['c', 'f', 'g', 'b'],\n    'b': ['v', 'g', 'h', 'n'],\n    'n': ['b', 'h', 'j', 'm'],\n    'm': ['n', 'j', 'k']\n}\n\ndef edit_distance(word1, word2):\n    \"\"\"Levenshtein distance with lower cost for keyboard typos & missing letters.\"\"\"\n    len1, len2 = len(word1), len(word2)\n    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n\n    # Initialize base cases\n    for i in range(len1 + 1):\n        dp[i][0] = i\n    for j in range(len2 + 1):\n        dp[0][j] = j\n\n    # Compute edit distances\n    for i in range(1, len1 + 1):\n        for j in range(1, len2 + 1):\n            cost = 1\n            if word1[i - 1] == word2[j - 1]:  \n                cost = 0  # No cost for exact match\n            elif word1[i - 1] in keyboard_neighbors and word2[j - 1] in keyboard_neighbors[word1[i - 1]]:\n                cost = 0.5  # Lower cost for adjacent keyboard typo\n            \n            # Check for a missing letter case\n            elif word1[:i] + word2[j - 1] + word1[i:] == word2:\n                cost = 0.5  # Missing letter penalty should be low\n\n            dp[i][j] = min(\n                dp[i - 1][j] + 1,    # Deletion\n                dp[i][j - 1] + 0.5,  # Insertion\n                dp[i - 1][j - 1] + cost   # Substitution\n            )\n\n    return dp[len1][len2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:43:15.034089Z","iopub.execute_input":"2025-03-04T20:43:15.034469Z","iopub.status.idle":"2025-03-04T20:43:15.074196Z","shell.execute_reply.started":"2025-03-04T20:43:15.034439Z","shell.execute_reply":"2025-03-04T20:43:15.072927Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 4. Generate Candidate Corrections  \n\n- **Function:** `get_candidates(word, dictionary)`  \n  - Generates possible corrections for a given word using the expanded dictionary.  \n\n- **Correction Process:**  \n  - If the word exists in the dictionary, it is returned as the only candidate.  \n  - Otherwise, potential candidates are generated by computing the **edit distance** between the input word and dictionary words (limited to words with length difference ≤ 2).  \n  - The closest matches are selected based on **minimum edit distance**.  \n\n- **Candidate Ranking:**  \n  - Candidates are ranked by their frequency in the corpus (defaulting to `1` if absent).  \n  - The function returns the **top 5 most probable corrections**.  ","metadata":{}},{"cell_type":"code","source":"def get_candidates(word, dictionary):\n    \"\"\"Generate corrections from expanded dictionary.\"\"\"\n    if word in dictionary:\n        return {word}\n\n    # Generate candidates from broader dictionary\n    candidates = {w: edit_distance(word, w) for w in dictionary if abs(len(w) - len(word)) <= 2}\n    \n    # Get best candidates with minimum edit distance\n    min_distance = min(candidates.values(), default=2)\n    best_candidates = {w for w, d in candidates.items() if d <= min_distance}\n\n    # Rank by frequency (fallback to 1 if word not in corpus)\n    best_candidates = sorted(best_candidates, key=lambda w: -word_frequencies.get(w, 1))\n\n    return best_candidates[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:43:25.599755Z","iopub.execute_input":"2025-03-04T20:43:25.600084Z","iopub.status.idle":"2025-03-04T20:43:25.606100Z","shell.execute_reply.started":"2025-03-04T20:43:25.600060Z","shell.execute_reply":"2025-03-04T20:43:25.604906Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 5. Compute Word Similarity Using SpaCy Embeddings  \n\n- **Pre-trained Embeddings:**  \n  - The **medium-sized** SpaCy model (`en_core_web_md`) is loaded to provide word vector representations.  \n\n- **Function:** `word_similarity(word1, word2)`  \n  - Computes semantic similarity between two words using **SpaCy word embeddings**.  \n  - If both words exist in the model's vocabulary, their **cosine similarity** is returned.  \n  - If either word is **out-of-vocabulary (OOV)**, a default similarity of `0` is assigned.  \n\n- **Use Case:**  \n  - Helps refine word correction by considering **semantic similarity** rather than just edit distance.  ","metadata":{}},{"cell_type":"code","source":"import spacy\n!python -m spacy download en_core_web_md -q\n\nnlp = spacy.load(\"en_core_web_md\")\n\ndef word_similarity(word1, word2):\n    \"\"\"Compute similarity between two words using SpaCy embeddings.\"\"\"\n    token1, token2 = nlp(word1), nlp(word2)\n    \n    # Check if words exist in the model's vocabulary\n    if token1.has_vector and token2.has_vector:\n        return token1.similarity(token2)\n    return 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:45:15.804039Z","iopub.execute_input":"2025-03-04T20:45:15.804434Z","iopub.status.idle":"2025-03-04T20:45:27.116541Z","shell.execute_reply.started":"2025-03-04T20:45:15.804393Z","shell.execute_reply":"2025-03-04T20:45:27.115503Z"}},"outputs":[{"name":"stdout","text":"\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 6. Compute Context Probability Using GPU  \n\n- **Function:** `get_best_correction(prev_word, word, next_word, dictionary)`  \n  - Determines the best possible correction for a word by evaluating **contextual probability**.  \n  - Incorporates **n-gram frequency data**, **edit distance penalties**, and **semantic similarity**.  \n\n- **Scoring Components:**  \n  - **Bigram Score:** Frequency of `(prev_word, candidate)`, with a small smoothing factor (`+0.1`).  \n  - **Fivegram Score:** Frequency of `(prev_word, candidate, next_word)`, also smoothed (`+0.1`).  \n  - **Frequency Score:** Word occurrence in the corpus (fallback to `1` if missing).  \n  - **Edit Penalty:** A **scaled-down** Levenshtein distance (`*0.1`), discouraging large modifications.  \n  - **Semantic Similarity:** Computed using **SpaCy embeddings** to boost candidates **closer in meaning** to `next_word`.  \n\n- **Final Ranking:**  \n  - The correction with the **highest total score** is selected, balancing **context, frequency, and similarity**.  \n  - Adjusts for **spelling errors** while maintaining **semantic and grammatical coherence**.  ","metadata":{}},{"cell_type":"code","source":"def get_best_correction(prev_word, word, next_word, dictionary):\n    \"\"\"Select best correction, including words outside the corpus.\"\"\"\n    candidates = get_candidates(word, dictionary)\n    best_word = word\n    max_score = -1\n\n    for candidate in candidates:\n        bigram_score = bigrams.get((prev_word, candidate), 0) + 0.1\n        fivegram_score = fivegrams.get((prev_word, candidate, next_word), 0) + 0.1\n        freq_score = word_frequencies.get(candidate, 1)\n        edit_penalty = edit_distance(word, candidate) * 0.1\n    \n        # Contextual Similarity using SpaCy\n        similarity_boost = word_similarity(candidate, next_word) * 2  # Increase weight\n    \n        total_score = (bigram_score * 0.6) + (fivegram_score * 0.6) + similarity_boost - edit_penalty\n        # total_score = (bigram_score * 0.6) + (fivegram_score * 0.6) + (freq_score * 0.001) + similarity_boost - edit_penalty\n\n        # print(candidate, total_score, bigram_score, fivegram_score, freq_score, similarity_boost, edit_penalty)\n    \n        if total_score > max_score:\n            max_score = total_score\n            best_word = candidate\n\n    return best_word","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:46:38.526635Z","iopub.execute_input":"2025-03-04T20:46:38.527099Z","iopub.status.idle":"2025-03-04T20:46:38.534018Z","shell.execute_reply.started":"2025-03-04T20:46:38.527066Z","shell.execute_reply":"2025-03-04T20:46:38.532970Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 7. Correct a Sentence Using GPU  \n\n- **Function:** `correct_sentence(sentence, dictionary)`  \n  - Iterates through a sentence word-by-word.  \n  - Identifies **misspelled words** and generates **candidate corrections**.  \n  - Selects the **best correction** using **context-aware probability and semantic similarity**.  \n\n- **Process:**  \n  1. **Tokenize Sentence:** Splits into individual words.  \n  2. **Determine Context:** Identifies the **previous (`prev_word`)** and **next (`next_word`)** word.  \n  3. **Generate Candidates:** Calls `get_candidates()` to find possible corrections.  \n  4. **Select Best Correction:** Uses `get_best_correction()` to choose the most probable replacement.  \n  5. **Reconstruct Sentence:** Joins corrected words into a final output.  ","metadata":{}},{"cell_type":"code","source":"def correct_sentence(sentence, dictionary):\n    \"\"\"Corrects a sentence and outputs all candidate corrections.\"\"\"\n    words = sentence.split()\n    corrected = []\n    \n    for i, word in enumerate(words):\n        prev_word = words[i-1] if i > 0 else \"<s>\"\n        next_word = words[i+1] if i < len(words) - 1 else \"</s>\"\n        \n        candidates = get_candidates(word, dictionary)\n        # print(f\"Word: {word}\\nCandidates: {candidates}\\n\")\n        \n        corrected.append(get_best_correction(prev_word, word, next_word, dictionary))\n    \n    return ' '.join(corrected)\n\n# Example Test Case\nsentence = \"dking species\"\ncorrected_sentence = correct_sentence(sentence, dictionary)\nprint(\"Original:\", sentence)\nprint(\"Corrected:\", corrected_sentence)\n\nsentence = \"dking sport\"\ncorrected_sentence = correct_sentence(sentence, dictionary)\nprint(\"Original:\", sentence)\nprint(\"Corrected:\", corrected_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:47:14.985197Z","iopub.execute_input":"2025-03-04T20:47:14.985585Z","iopub.status.idle":"2025-03-04T20:47:24.673135Z","shell.execute_reply.started":"2025-03-04T20:47:14.985553Z","shell.execute_reply":"2025-03-04T20:47:24.672049Z"}},"outputs":[{"name":"stdout","text":"Original: dking species\nCorrected: dying species\nOriginal: dking sport\nCorrected: eking sport\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"sentence = \"i live a whoke hapoy life\"\ncorrected_sentence = correct_sentence(sentence, dictionary)\nprint(\"Original:\", sentence)\nprint(\"Corrected:\", corrected_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:47:42.650799Z","iopub.execute_input":"2025-03-04T20:47:42.651179Z","iopub.status.idle":"2025-03-04T20:47:52.035600Z","shell.execute_reply.started":"2025-03-04T20:47:42.651149Z","shell.execute_reply":"2025-03-04T20:47:52.034495Z"}},"outputs":[{"name":"stdout","text":"Original: i live a whoke hapoy life\nCorrected: i live a whole happy life\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## 8. Prepare test data for evaluation \n\n- **Goal:** Simulate **real-world typos** by randomly modifying words in a controlled way.  \n\n### **Functions:**  \n- `introduce_noise(word, noise_prob=0.8)`:  \n  - Randomly applies **one of four typo types**:  \n    - **Substitution:** Replace a letter with a random one.  \n    - **Deletion:** Remove a random letter.  \n    - **Insertion:** Add a random letter.  \n    - **Transposition:** Swap adjacent letters.  \n  - Only applies noise with **80% probability** and avoids very short words.  \n\n- `introduce_noise_sentence(sentence, noise_prob=0.5)`:  \n  - Introduces typos to words in a sentence with **50% probability per word**.","metadata":{}},{"cell_type":"code","source":"import random\n\nvocab = list(set(words.words()))\n\ndef introduce_noise(word, noise_prob=0.8):\n    \"\"\"Introduce typos into a word with a given probability.\"\"\"\n    if len(word) < 3 or random.random() > noise_prob:\n        return word\n    \n    word = list(word)\n    typo_type = random.choice(['substitute', 'delete', 'insert', 'transpose'])\n\n    if typo_type == 'substitute':  # Replace with a random letter\n        idx = random.randint(0, len(word) - 1)\n        word[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n\n    elif typo_type == 'delete':  # Remove a random character\n        idx = random.randint(0, len(word) - 1)\n        word.pop(idx)\n\n    elif typo_type == 'insert':  # Insert a random character\n        idx = random.randint(0, len(word))\n        word.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n\n    elif typo_type == 'transpose' and len(word) > 1:  # Swap two adjacent letters\n        idx = random.randint(0, len(word) - 2)\n        word[idx], word[idx + 1] = word[idx + 1], word[idx]\n\n    return ''.join(word)\n\ndef introduce_noise_sentence(sentence, noise_prob=0.5):\n    \"\"\"Introduce typos into words in a sentence.\"\"\"\n    words = sentence.split()\n    noisy_words = [introduce_noise(word) if random.random() < noise_prob else word for word in words]\n    return ' '.join(noisy_words)\n\n# Generate test sets\ntest_words = [\"brisk\", \"tangle\", \"mirth\", \"serene\", \"loud\", \"swift\", \"gaze\", \"hush\", \"bloom\", \"chill\", \"dking\"]\ntest_sentences = [\n    \"Cats sleep a lot.\",  \n    \"The wind feels cold.\",  \n    \"She smiled softly.\",  \n    \"Birds fly high.\",  \n    \"Music calms me.\",  \n    \"The sky turned red.\",  \n    \"I lost my keys.\",  \n    \"Rain is coming.\",  \n    \"Lights flickered fast.\",  \n    \"Snow covered everything.\",  \n    \"I love doing sport!\",  \n    \"Ther are dying species.\"\n]\n\n# Introduce noise\nnoisy_words = [introduce_noise(w) for w in test_words]\nnoisy_sentences = [introduce_noise_sentence(s) for s in test_sentences]\ntest_words += [\"doing sport\", \"dying species\"]\nnoisy_words += [\"dking sport\", \"dking species\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:51:06.411055Z","iopub.execute_input":"2025-03-04T20:51:06.411406Z","iopub.status.idle":"2025-03-04T20:51:06.553906Z","shell.execute_reply.started":"2025-03-04T20:51:06.411379Z","shell.execute_reply":"2025-03-04T20:51:06.553106Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## 9. Norvig Solution \n(code is taken from https://norvig.com/spell-correct.html, corpus is taken from https://norvig.com/big.txt)","metadata":{}},{"cell_type":"code","source":"import urllib.request\n\nurl = \"https://norvig.com/big.txt\"\nfilename = \"big.txt\"\n\nurllib.request.urlretrieve(url, filename)\nprint(\"Download complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:54:42.791329Z","iopub.execute_input":"2025-03-04T20:54:42.791665Z","iopub.status.idle":"2025-03-04T20:54:45.026912Z","shell.execute_reply.started":"2025-03-04T20:54:42.791640Z","shell.execute_reply":"2025-03-04T20:54:45.025947Z"}},"outputs":[{"name":"stdout","text":"Download complete!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import re\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\nWORDS = Counter(words(open('/kaggle/working/big.txt').read()))\n\ndef P(word, N=sum(WORDS.values())): \n    \"Probability of `word`.\"\n    return WORDS[word] / N\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:54:46.695391Z","iopub.execute_input":"2025-03-04T20:54:46.695754Z","iopub.status.idle":"2025-03-04T20:54:47.238477Z","shell.execute_reply.started":"2025-03-04T20:54:46.695727Z","shell.execute_reply":"2025-03-04T20:54:47.237636Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## 10. Evaluate and compare Spell Correctors","metadata":{}},{"cell_type":"code","source":"# Evaluate Correctors\ndef evaluate_corrector(corrector, test_words, noisy_words):\n    \"\"\"Evaluate spelling corrector on a test set.\"\"\"\n    correct_count = sum(1 for w, nw in zip(test_words, noisy_words) if corrector(nw) == w)\n    return correct_count / len(test_words)\n\n# Compare Correctors\ndef compare_correctors(test_words, noisy_words):\n    \"\"\"Compare custom corrector and Norvig's corrector.\"\"\"\n    results = []\n    for word, noisy_word in zip(test_words, noisy_words):\n        custom_corrected = correct_sentence(noisy_word, dictionary)\n        norvig_corrected = correction(noisy_word)\n        results.append((noisy_word, custom_corrected, norvig_corrected))\n    return results\n\n# Print Results as a Table\ndef print_results_table(results):\n    \"\"\"Print comparison results in a table format.\"\"\"\n    print(\"{:<20} {:<20} {:<20}\".format(\"Noisy Word\", \"Custom Corrector\", \"Norvig Corrector\"))\n    print(\"-\" * 60)\n    for noisy_word, custom_corrected, norvig_corrected in results:\n        print(\"{:<20} {:<20} {:<20}\".format(noisy_word, custom_corrected, norvig_corrected))\n\nresults = compare_correctors(test_words, noisy_words)\nprint_results_table(results)\naccuracy_custom = evaluate_corrector(lambda w: correct_sentence(w, dictionary), test_words, noisy_words)\naccuracy_norvig = evaluate_corrector(correction, test_words, noisy_words)\n\nprint(f\"\\nCustom Corrector Accuracy: {accuracy_custom:.2%}\")\nprint(f\"Norvig's Corrector Accuracy: {accuracy_norvig:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T20:56:43.572604Z","iopub.execute_input":"2025-03-04T20:56:43.573089Z","iopub.status.idle":"2025-03-04T20:57:54.209331Z","shell.execute_reply.started":"2025-03-04T20:56:43.573057Z","shell.execute_reply":"2025-03-04T20:57:54.208293Z"}},"outputs":[{"name":"stdout","text":"Noisy Word           Custom Corrector     Norvig Corrector    \n------------------------------------------------------------\nbrisk                brisk                brisk               \ntnagle               tenable              tangle              \nmitrh                mitre                mirth               \nerene                serene               serene              \nloud                 loud                 loud                \nswift                swift                swift               \naze                  ase                  are                 \nhsh                  hash                 hush                \nloom                 loom                 loom                \nchill                chill                chill               \ndkhng                dining               king                \ndking sport          eking sport          dking sport         \ndking species        dying species        dking species       \n\nCustom Corrector Accuracy: 46.15%\nNorvig's Corrector Accuracy: 61.54%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Justification for a custom solution","metadata":{}},{"cell_type":"markdown","source":"### Features Implemented:\n\n#### N-gram Language Model:\n- The solution uses **bigrams** and **fivegrams** to capture contextual information. This allows the model to consider the surrounding words when making corrections, which is crucial for context-sensitive spelling correction.\n- The n-gram frequencies are loaded from the provided dataset (`bigrams.txt` and `fivegrams.txt`), which ensures that the model is trained on a large and diverse corpus.\n\n#### Edit Distance with Keyboard Neighbors:\n- The Levenshtein distance is modified to account for **keyboard typos**. By assigning a lower cost (0.5) to substitutions involving adjacent keys on a QWERTY keyboard, the model is more forgiving of common typing errors.\n- This feature helps the model prioritize corrections that are more likely to be actual typos (e.g., \"dking\" → \"doing\" or \"dying\").\n\n#### Semantic Similarity with SpaCy Embeddings:\n- The solution incorporates **SpaCy word embeddings** to measure semantic similarity between words. This helps the model choose corrections that are not only contextually appropriate but also semantically coherent.\n- For example, in the phrase \"dking species,\" the model correctly identifies \"dying\" as the most appropriate correction because it aligns semantically with \"species.\"\n\n#### Candidate Generation and Ranking:\n- The model generates candidate corrections by considering words within an edit distance of 2 and ranks them based on a combination of **n-gram frequency**, **edit distance penalty**, and **semantic similarity**.\n- This multi-faceted approach ensures that the most probable correction is selected, balancing context, frequency, and meaning.\n\n#### Efficiency Improvements:\n- The n-gram frequencies are moved to the GPU using **PyTorch** to speed up computations. This is particularly useful when dealing with large datasets, as it reduces the time required for probability calculations.\n- The use of **defaultdict** for storing n-gram frequencies also helps in efficiently managing memory and access times.\n\n### Problems and how the solution resolved them:\n\n#### Large N-gram Dataset:\n- Storing and processing large n-gram datasets can be computationally expensive. To address this, the solution uses **GPU acceleration** with PyTorch to speed up the computation of n-gram probabilities.\n- Additionally, the use of **defaultdict** ensures that only relevant n-grams are stored, reducing memory overhead.\n\n#### Out-of-Vocabulary Words:\n- The model handles out-of-vocabulary (OOV) words by generating candidate corrections based on edit distance and semantic similarity. If a word is not found in the dictionary, the model still attempts to correct it by considering nearby words in the embedding space.\n- This approach ensures that the model can handle rare or misspelled words effectively.\n\n#### Keyboard Layout and Typos:\n- The solution incorporates a **keyboard neighbor mapping** to account for common typing errors. By assigning a lower cost to substitutions involving adjacent keys, the model is better equipped to handle real-world typos.\n- This feature is particularly useful for correcting words like \"dking\" to \"doing\" or \"dying,\" depending on the context.\n\n#### Semantic Coherence:\n- To ensure that corrections are not only contextually but also semantically appropriate, the solution uses **SpaCy embeddings** to measure word similarity. This helps the model choose corrections that make sense in the given context.\n- For example, in the phrase \"dking species,\" the model correctly identifies \"dying\" as the most appropriate correction because it aligns semantically with \"species.\"\n\n### Comparison of Norvig solution with Custom one:\n\n#### Custom Corrector:\n- The custom corrector achieves an accuracy of **46.15%** on the test set. While this is lower than Norvig's corrector, it demonstrates the effectiveness of incorporating **contextual information** (via n-grams) and **semantic similarity** (via SpaCy embeddings).\n- The custom corrector is particularly strong in handling **context-sensitive corrections**, such as distinguishing between \"dking sport\" and \"dking species.\" It also performs well in correcting **keyboard typos** due to the modified edit distance algorithm.\n\n#### Norvig's Corrector:\n- Norvig's corrector achieves a higher accuracy of **61.54%** on the test set. This is expected, as Norvig's solution is a well-established and highly optimized spell-checking algorithm.\n- However, Norvig's corrector lacks the **contextual awareness** and **semantic understanding** of the custom solution. For example, it fails to correct \"dking sport\" to \"doing sport\" or \"dking species\" to \"dying species,\" as it does not consider the surrounding words or semantic coherence.\n","metadata":{}},{"cell_type":"markdown","source":"#### Useful resources (also included in the archive in moodle):\n\n1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)","metadata":{}}]}